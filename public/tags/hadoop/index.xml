<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>hadoop on Fatkun&#39;s Blog</title>
    <link>http://fatkun.github.io/tags/hadoop/</link>
    <description>Recent content in hadoop on Fatkun&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Sun, 24 Dec 2017 10:15:12 +0000</lastBuildDate>
    
	<atom:link href="http://fatkun.github.io/tags/hadoop/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>使用docker部署hadoop hdfs</title>
      <link>http://fatkun.github.io/2017/12/deploy-hadoop-hdfs-use-docker.html</link>
      <pubDate>Sun, 24 Dec 2017 10:15:12 +0000</pubDate>
      
      <guid>http://fatkun.github.io/2017/12/deploy-hadoop-hdfs-use-docker.html</guid>
      <description>先创建一个网络 docker network create hadoop 把本机新建一个目录hadoop，以下xml保存为docker-compose.yml version: &#34;3&#34; services: namenode: image: uhopper/hadoop-namenode hostname: namenode1 container_name: namenode1 # 要和netw</description>
    </item>
    
    <item>
      <title>Hive On Tez cloudera5.4</title>
      <link>http://fatkun.github.io/2017/05/hive-on-tez-cloudera5-4.html</link>
      <pubDate>Mon, 01 May 2017 10:52:17 +0000</pubDate>
      
      <guid>http://fatkun.github.io/2017/05/hive-on-tez-cloudera5-4.html</guid>
      <description>配置 tez-site.xml &amp;lt;configuration&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;tez.lib.uris&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;${fs.defaultFS}/apps/tez-0.8.5/,${fs.defaultFS}/apps/tez-0.8.5/lib/&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;tez.use.cluster.hadoop-libs&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;tez.runtime.compress&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;tez.runtime.compress.codec&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;org.apache.hadoop.io.compress.SnappyCodec&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;/configuration&amp;gt; 调优参数 tez.grouping.min-size 分片最小限制 报错处理 找不到lzo Caused by: java.lang.ClassNotFoundException: Class com.hadoop.compression.lzo.LzoCodec not found at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2018) at org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses(CompressionCodecFactory.java:128) ... 23 more LZO包没加载到，我是把lz</description>
    </item>
    
    <item>
      <title>hadoop组映射从配置文件读取</title>
      <link>http://fatkun.github.io/2016/07/hadoop-group-mapping-from-file.html</link>
      <pubDate>Sun, 31 Jul 2016 10:41:41 +0000</pubDate>
      
      <guid>http://fatkun.github.io/2016/07/hadoop-group-mapping-from-file.html</guid>
      <description>hadoop实现类似linux系统的文件权限，需要知道某个用户是属于哪个组。系统默认是使用ShellBasedUnixGroupsMappi</description>
    </item>
    
    <item>
      <title>hadoop磁盘写入选择策略</title>
      <link>http://fatkun.github.io/2016/07/hadoop-volume-choosing-policy.html</link>
      <pubDate>Thu, 14 Jul 2016 10:27:36 +0000</pubDate>
      
      <guid>http://fatkun.github.io/2016/07/hadoop-volume-choosing-policy.html</guid>
      <description>hadoop写入时，默认磁盘选择是轮询的方式。可以改用按空间大小来分配，避免某个磁盘特别满。 参数 ：dfs.datanode.fsdatase</description>
    </item>
    
    <item>
      <title>[HADOOP-11238]hdfs namenode getGroups延迟</title>
      <link>http://fatkun.github.io/2016/05/hadoop-11238.html</link>
      <pubDate>Sun, 15 May 2016 02:29:06 +0000</pubDate>
      
      <guid>http://fatkun.github.io/2016/05/hadoop-11238.html</guid>
      <description>在namenode日志看到如下错误 2016-05-11 01:00:26,360 WARN org.apache.hadoop.security.Groups: Potential performance problem: getGroups(user=xxx) took 5046 milliseconds 这个方法是hadoop为了获取某个用户是哪个组。 分析补丁 补丁：https://iss</description>
    </item>
    
    <item>
      <title>DataNode启动流程源码分析</title>
      <link>http://fatkun.github.io/2016/05/datanode-start.html</link>
      <pubDate>Mon, 02 May 2016 09:28:42 +0000</pubDate>
      
      <guid>http://fatkun.github.io/2016/05/datanode-start.html</guid>
      <description>背景 最近打算要重启DataNode，之前有试过重启过程中导致业务任务失败的情况。所以想了解DataNode什么时候才算启动完成，以及能否检测</description>
    </item>
    
    <item>
      <title>HDFS-8824 平衡数据的时候不移动小文件</title>
      <link>http://fatkun.github.io/2016/05/hdfs-8824.html</link>
      <pubDate>Sun, 01 May 2016 02:08:22 +0000</pubDate>
      
      <guid>http://fatkun.github.io/2016/05/hdfs-8824.html</guid>
      <description>Do not use small blocks for balancing the cluster https://issues.apache.org/jira/browse/HDFS-8824 Allow Balancer to run faster https://issues.apache.org/jira/browse/HDFS-8818 平衡设置的文章：https://community.hortonworks.com/articles/438</description>
    </item>
    
    <item>
      <title>hdfs complete file 超时</title>
      <link>http://fatkun.github.io/2016/04/hdfs-complete-file-timeout.html</link>
      <pubDate>Mon, 25 Apr 2016 02:48:00 +0000</pubDate>
      
      <guid>http://fatkun.github.io/2016/04/hdfs-complete-file-timeout.html</guid>
      <description>修改一下超时次数，具体代码在 DFSOutputStream.java private void completeFile(ExtendedBlock last) throws IOException { long localstart = Time.now(); long localTimeout = 400; boolean fileComplete = false; int retries = dfsClient.getConf().nBlockWriteLocateFollowingRetry; while (!fileComplete) { fileComplete = dfsClient.namenode.complete(src, dfsClient.clientName, last, fileId); if (!fileComplete) { final int hdfsTimeout = dfsClient.getHdfsTimeout(); if (!dfsClient.clientRunning &amp;brvbar;&amp;brvbar; (hdfsTimeout &amp;gt; 0 &amp;&amp; localstart + hdfsTimeout &amp;lt; Time.now())) { String msg</description>
    </item>
    
    <item>
      <title>hdfs启动相关文章</title>
      <link>http://fatkun.github.io/2016/04/hdfs-start.html</link>
      <pubDate>Sun, 24 Apr 2016 08:13:38 +0000</pubDate>
      
      <guid>http://fatkun.github.io/2016/04/hdfs-start.html</guid>
      <description>记一次DataNode慢启动问题 ：启动过程中，datanode为了获取used size，如果超过时间，可能会执行DU。</description>
    </item>
    
    <item>
      <title>Container killed by the ApplicationMaster, Exit code is 143</title>
      <link>http://fatkun.github.io/2015/12/container-killed-by-the-applicationmaster.html</link>
      <pubDate>Wed, 30 Dec 2015 13:14:03 +0000</pubDate>
      
      <guid>http://fatkun.github.io/2015/12/container-killed-by-the-applicationmaster.html</guid>
      <description>之前发现在map任务里面经常看到Container killed by the ApplicationMaster，挺奇怪，不过任务最终是成功的，就没怎么管。不过最</description>
    </item>
    
    <item>
      <title>hdfs exceeds the limit of concurrent xcievers</title>
      <link>http://fatkun.github.io/2015/11/hdfs-exceeds-the-limit-of-concurrent-xcievers.html</link>
      <pubDate>Mon, 23 Nov 2015 11:53:49 +0000</pubDate>
      
      <guid>http://fatkun.github.io/2015/11/hdfs-exceeds-the-limit-of-concurrent-xcievers.html</guid>
      <description>版本: hadoop cdh5.4 datanode jstack，很多这样的线程 &#34;DataXceiver for client unix:/var/run/hdfs-sockets/dn [Waiting for operation #1]&#34; daemon prio=10 tid=0x00007ffc42de9000 nid=0x68f8 waiting on condition [0x00007ffacbd1d000] java.lang.Thread.State: WAITING (parking) at sun.misc.Unsafe.park(Native Method) - parking to wait for &amp;lt;0x00000007a5d3d568&amp;gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject) at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186) at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043) at org.apache.hadoop.net.unix.DomainSocketWatcher.add(DomainSocketWatcher.java:316) at org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry.createNewMemorySegment(ShortCircuitRegistry.java:322) at org.apache.hadoop.hdfs.server.datanode.DataXceiver.requestShortCircuitShm(DataXceiver.java:418) at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opRequestShortCircuitShm(Receiver.java:214) at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:95) at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:241) at java.lang.Thread.run(Thread.java:745) 相关</description>
    </item>
    
    <item>
      <title>Mismatched address stored in ZK for NameNode</title>
      <link>http://fatkun.github.io/2015/10/mismatched-address-stored-in-zk-for-namenode.html</link>
      <pubDate>Wed, 28 Oct 2015 08:28:52 +0000</pubDate>
      
      <guid>http://fatkun.github.io/2015/10/mismatched-address-stored-in-zk-for-namenode.html</guid>
      <description>我在namenode配置dfs.namenode.servicerpc-address端口后，Failover Controller 报错 java.lang.RuntimeException: Mismatched address stored in ZK for NameNode at kpixxx/xx.xx.xx.xx:88 解决</description>
    </item>
    
    <item>
      <title>在Windows上编译hadoop cdh5.4</title>
      <link>http://fatkun.github.io/2015/05/compile-hadoop-cdh5-4-on-windows.html</link>
      <pubDate>Sun, 17 May 2015 14:07:27 +0000</pubDate>
      
      <guid>http://fatkun.github.io/2015/05/compile-hadoop-cdh5-4-on-windows.html</guid>
      <description>我的环境 系统:win8.1 ， maven3.1, eclipse 4.4 hadoop cdh5.4对应apache的源码是hadoop 2.6 准备条件 安装以下软件，见源码src/BUILDING.</description>
    </item>
    
    <item>
      <title>cloudera manager添加hive时报错找不到jdbc driver</title>
      <link>http://fatkun.github.io/2015/05/cloudera-manage-jdbc-driver-cannot-be-found.html</link>
      <pubDate>Thu, 07 May 2015 03:19:57 +0000</pubDate>
      
      <guid>http://fatkun.github.io/2015/05/cloudera-manage-jdbc-driver-cannot-be-found.html</guid>
      <description>cm 4.7 报错 JDBC driver cannot be found. Unable to find the JDBC database jar on host 解决方法 把包放入这个目录，注意文件名要保持一致 /usr/share/java/mysql-connector-java.jar 来源 https://groups.google.com/a/cloudera.org/forum/#!topic/cdh-user/OpXSfmzsnuo</description>
    </item>
    
    <item>
      <title>namenode standby checkponit时间过长导致的问题</title>
      <link>http://fatkun.github.io/2015/05/namenode-standby-checkponit.html</link>
      <pubDate>Sun, 03 May 2015 13:30:28 +0000</pubDate>
      
      <guid>http://fatkun.github.io/2015/05/namenode-standby-checkponit.html</guid>
      <description>我们当前使用的版本是cdh-4.2.1，standby namenode默认每小时生成一个editlog文件，由于操作很多，这个保存时间超过了</description>
    </item>
    
    <item>
      <title>impala因为invalidate语句导致执行DDL语句卡住</title>
      <link>http://fatkun.github.io/2015/02/impala-invalidate-metadata.html</link>
      <pubDate>Sun, 01 Feb 2015 16:06:11 +0000</pubDate>
      
      <guid>http://fatkun.github.io/2015/02/impala-invalidate-metadata.html</guid>
      <description>现象 impala突然无法执行DDL语句，一直卡住，累积的很多任务。初步怀疑impala并发有问题，采取了一些减少并发的方法，有一点作用，但故</description>
    </item>
    
    <item>
      <title>从日志分析impala查询慢的原因（2）</title>
      <link>http://fatkun.github.io/2015/01/analyze-slow-impala-2.html</link>
      <pubDate>Sat, 03 Jan 2015 08:51:57 +0000</pubDate>
      
      <guid>http://fatkun.github.io/2015/01/analyze-slow-impala-2.html</guid>
      <description>上一篇文章看到 requestTblLoadAndWait 速度缓慢，继续分析。先上日志 # impalad日志 I0103 15:09:35.503356 27319 impala-beeswax-server.cc:170] query(): query=select distinct(disp_id) from default_impala.kpi_disp_user_info_stat_tb where pt=&#39;2014-04-08&#39; I0103 15:11:36.109915 27319 Frontend.java:779] Missing tables were not received in 120000ms. Load request will be retried. I0103 15:11:36.110705 27319 Frontend.java:709] Requesting prioritized load of table(s): default_impala.kpi_disp_user_info_stat_tb I0103</description>
    </item>
    
    <item>
      <title>hdfs httpfs与webhdfs的简单使用</title>
      <link>http://fatkun.github.io/2014/11/httpfs-and-webhdfs.html</link>
      <pubDate>Sun, 02 Nov 2014 08:46:33 +0000</pubDate>
      
      <guid>http://fatkun.github.io/2014/11/httpfs-and-webhdfs.html</guid>
      <description>HttpFS和WebHDFS 通过http协议操作hdfs有两个组件，httpfs和webhdfs，我一开始还以为这两个是同一个东西，其实不是</description>
    </item>
    
    <item>
      <title>hadoop本地目录相关代码分析</title>
      <link>http://fatkun.github.io/2014/09/hadoop-local-dir.html</link>
      <pubDate>Thu, 04 Sep 2014 10:23:12 +0000</pubDate>
      
      <guid>http://fatkun.github.io/2014/09/hadoop-local-dir.html</guid>
      <description>最近hadoop本地磁盘总是坏，伴随着有些hadoop job失败，阅读了一些相关的代码。 本地磁盘健康检查 NodeManager默认会每两分钟</description>
    </item>
    
    <item>
      <title>hadoop重启Namenode时，appTokens报FileNotFoundException</title>
      <link>http://fatkun.github.io/2014/07/hadoop-apptokens-filenotfoundexception.html</link>
      <pubDate>Sun, 27 Jul 2014 13:56:09 +0000</pubDate>
      
      <guid>http://fatkun.github.io/2014/07/hadoop-apptokens-filenotfoundexception.html</guid>
      <description>现象 报错如下 Application application_1405852606905_0014 failed 3 times due to AM Container for appattempt_1405852606905_0014_000003 exited with exitCode: -1000 due to: RemoteTrace: java.io.FileNotFoundException: File does not exist: hdfs://mycluster:8020/user/kpi/.staging/job_1405852606905_0014/appTokens at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:809) 同时注意到是因为每次重启nodemanager才发生。 首先用关键词“appt</description>
    </item>
    
    <item>
      <title>【转】HADOOP 2.0 YARN中的MR/RM/NM状态转换图</title>
      <link>http://fatkun.github.io/2014/03/hadoop-2-0-yarn-state.html</link>
      <pubDate>Sun, 23 Mar 2014 07:36:48 +0000</pubDate>
      
      <guid>http://fatkun.github.io/2014/03/hadoop-2-0-yarn-state.html</guid>
      <description>hadoop2 可以用GRAPHVIZ来生成状态图，点击下面的连接查看状态转换图。 http://www.rigongyizu.com/generate-hadoop-yarn-state-transit-graph/</description>
    </item>
    
    <item>
      <title>在eclipse远程调试mapreduce（hadoop2）</title>
      <link>http://fatkun.github.io/2014/02/debug-mapreduce-with-eclipse.html</link>
      <pubDate>Sat, 22 Feb 2014 08:13:20 +0000</pubDate>
      
      <guid>http://fatkun.github.io/2014/02/debug-mapreduce-with-eclipse.html</guid>
      <description>背景 线上的服务器都是linux，直接在线上debug不需要准备太多测试数据。 实现 远程调试实际上实在服务端开启一个监听端口，通过eclipse</description>
    </item>
    
    <item>
      <title>hadoop wordcount新API例子</title>
      <link>http://fatkun.github.io/2013/10/hadoop-wordcount-new-api.html</link>
      <pubDate>Sun, 27 Oct 2013 16:26:48 +0000</pubDate>
      
      <guid>http://fatkun.github.io/2013/10/hadoop-wordcount-new-api.html</guid>
      <description>准备 准备一些输入文件，可以用hdfs dfs -put xxx/* /user/fatkun/input上传文件 代码 package com.fatkun; import java.io.IOException; import java.util.ArrayList; import java.util.List; import java.util.StringTokenizer; import org.apache.commons.logging.Log; import org.apache.commons.logging.LogFactory; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.conf.Configured; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.LongWritable; import</description>
    </item>
    
    <item>
      <title>在windows部署nexus编译hadoop cdh4.3</title>
      <link>http://fatkun.github.io/2013/10/maven-nexus.html</link>
      <pubDate>Mon, 14 Oct 2013 18:02:45 +0000</pubDate>
      
      <guid>http://fatkun.github.io/2013/10/maven-nexus.html</guid>
      <description>部署nexus 死活启动不了nexus，下载了多个版本。最后下载tomcat7 + 2.5的war才可以。把war包丢到webapps目录下，启动</description>
    </item>
    
    <item>
      <title>hive命令行抛NullPointerException</title>
      <link>http://fatkun.github.io/2012/07/hive-cmd-nullpointerexception.html</link>
      <pubDate>Wed, 18 Jul 2012 16:11:23 +0000</pubDate>
      
      <guid>http://fatkun.github.io/2012/07/hive-cmd-nullpointerexception.html</guid>
      <description>我只是执行了一下这个代码 create temporary function strip as &#39;com.fatkun.Strip&#39;; 就抛出异常 hive&amp;gt; create temporary function strip as &#39;com.fatkun.Strip&#39;; OK Exception in thread &#34;main&#34; java.lang.NullPointerException at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:221) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:286) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:516) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.hadoop.util.RunJar.main(RunJar.java:197) 从代码中看CliDriver.ja</description>
    </item>
    
    <item>
      <title>挂载fuse-dfs遇到的问题</title>
      <link>http://fatkun.github.io/2012/07/mount-fuse-dfs.html</link>
      <pubDate>Wed, 18 Jul 2012 15:27:01 +0000</pubDate>
      
      <guid>http://fatkun.github.io/2012/07/mount-fuse-dfs.html</guid>
      <description>以下是在ubuntu12.04 amd64， hadoop cdh3u3下安装 在${HADOOP_HOME}/contrib/fuse-dfs目录下已经帮我</description>
    </item>
    
    <item>
      <title>在Windows下编译hadoop eclipse-plugin</title>
      <link>http://fatkun.github.io/2012/07/compile-hadoop-eclipse-plugin-on-windows.html</link>
      <pubDate>Sat, 07 Jul 2012 09:06:01 +0000</pubDate>
      
      <guid>http://fatkun.github.io/2012/07/compile-hadoop-eclipse-plugin-on-windows.html</guid>
      <description>1.首先准备好ant和maven，配置好ANT_HOME和MAVEN_HOME的环境变量，把%ANT_HOME%/bin和%MAVEN/HO</description>
    </item>
    
    <item>
      <title>格式化Hive语法树(python)</title>
      <link>http://fatkun.github.io/2012/05/format-hive-astnode.html</link>
      <pubDate>Sat, 19 May 2012 17:19:02 +0000</pubDate>
      
      <guid>http://fatkun.github.io/2012/05/format-hive-astnode.html</guid>
      <description>为了容易看一点，把用explain得到的语法树加上一些缩进. 该代码只是简单的加上缩进. 效果 这是查询explain select key from kv mykv join test mytest on (mykv.key == myt</description>
    </item>
    
  </channel>
</rss>